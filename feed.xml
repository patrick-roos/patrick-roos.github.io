<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://patrick-roos.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://patrick-roos.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-02T02:41:17+00:00</updated><id>https://patrick-roos.github.io/feed.xml</id><title type="html">blank</title><subtitle>Patrick Roos Personal Website - Computer Science PhD, Artificial Intelligence, Machine Learning, Data Science </subtitle><entry><title type="html">Using PySpark and Pandas UDFs to Train Scikit-Learn Models Distributedly</title><link href="https://patrick-roos.github.io/blog/2019/pyspark-pandas-udf/" rel="alternate" type="text/html" title="Using PySpark and Pandas UDFs to Train Scikit-Learn Models Distributedly"/><published>2019-02-25T20:09:00+00:00</published><updated>2019-02-25T20:09:00+00:00</updated><id>https://patrick-roos.github.io/blog/2019/pyspark-pandas-udf</id><content type="html" xml:base="https://patrick-roos.github.io/blog/2019/pyspark-pandas-udf/"><![CDATA[<p>Say you find yourself in the peculiar situation where you need to train a whole bunch of scikit-learn models over different groups from a large amount of data. And say you want to leverage Spark to distribute the process to do it all in a scalable fashion.</p> <p>Recently I ran into such a use case and found that by using pandas_udf – a PySpark user defined function (UDF) made available through PyArrow – this can be done in a pretty straight-forward fashion. Pandas UDFs allow you to write a UDF that is just like a regular Spark UDF that operates over some grouped or windowed data, except it takes in data as a pandas DataFrame and returns back a pandas DataFrame. We just need to define the schema for the pandas DataFrame returned.</p> <p>Let’s define this return schema. Assume we have some group_id that we can use to group our data into those portions that will be used to train each model. We’ll return a model with that group_id and since it might good info to have later let’s also return the number of instances within that group that the model was trained with, call it num_instances_trained_with. To store all the trained models we will use the python pickle library to dump the model to a string which we can later load back, call it model_str.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># define schema for what the pandas udf will return
</span><span class="n">schema</span> <span class="o">=</span> <span class="nc">StructType</span><span class="p">([</span>
<span class="nc">StructField</span><span class="p">(</span><span class="sh">'</span><span class="s">group_id</span><span class="sh">'</span><span class="p">,</span> <span class="nc">IntegerType</span><span class="p">()),</span>
<span class="nc">StructField</span><span class="p">(</span><span class="sh">'</span><span class="s">num_instances_trained_with</span><span class="sh">'</span><span class="p">,</span> <span class="nc">IntegerType</span><span class="p">()),</span>
<span class="nc">StructField</span><span class="p">(</span><span class="sh">'</span><span class="s">model_str</span><span class="sh">'</span><span class="p">,</span> <span class="nc">StringType</span><span class="p">())</span>
<span class="p">])</span>
</code></pre></div></div> <p>To define a pandas UDF that will train a scikit-learn model, we need to use the pandas_udf decorator, and since we will take in a pandas DataFrame and return the same we need to define the function as a PandasUDFType.GROUPED_MAP (as opposed to PandasUDFType.SCALAR which would take just a pandas Series). Within the UDF we can then train a scikit-learn model using the data coming in as a pandas DataFrame, just like we would in a regular python application:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pickle</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">PandasUDFType</span>

<span class="nd">@pandas_udf</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="n">PandasUDFType</span><span class="p">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">df_pandas</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    Trains a RandomForestRegressor model on training instances
    in df_pandas.

    Assumes: df_pandas has the columns:
                 [</span><span class="sh">'</span><span class="s">my_feature_1</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">my_feature_2</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">my_label</span><span class="sh">'</span><span class="s">]

    Returns: a single row pandas DataFrame with columns:
               [</span><span class="sh">'</span><span class="s">group_id</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">num_instances_trained_with</span><span class="sh">'</span><span class="s">,
                </span><span class="sh">'</span><span class="s">model_str</span><span class="sh">'</span><span class="s">]
    </span><span class="sh">'''</span>

    <span class="c1"># get the value of this group id
</span>    <span class="n">group_id</span> <span class="o">=</span> <span class="n">df_pandas</span><span class="p">[</span><span class="sh">'</span><span class="s">group_id</span><span class="sh">'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># get the number of training instances for this group
</span>    <span class="n">num_instances</span> <span class="o">=</span> <span class="n">df_pandas</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># get features and label for all training instances in this group
</span>    <span class="n">feature_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">my_feature_1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">my_feature_2</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">my_label</span><span class="sh">'</span><span class="p">;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df_pandas</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">df_pandas</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>

    <span class="c1"># train this model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>

    <span class="c1"># get a string representation of our trained model to store
</span>    <span class="n">model_str</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># build the DataFrame to return
</span>    <span class="n">df_to_return</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">([</span><span class="n">group_id</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="n">model_str</span><span class="p">],</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">group_id</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">num_instances_trained_with</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">model_str</span><span class="sh">'</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">df_to_return</span>
</code></pre></div></div> <p>Now, assuming we have a PySpark DataFrame (df) with our features and labels and a group_id, we can apply this pandas UDF to all groups of our data and get back a PySpark DataFrame with a model trained (stored as a pickle dumped string) on the data for each group:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_trained_models</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">groupBy</span><span class="p">(</span><span class="sh">'</span><span class="s">group_id</span><span class="sh">'</span><span class="p">).</span><span class="nf">apply</span><span class="p">(</span><span class="n">train_model</span><span class="p">)</span>
</code></pre></div></div> <p>Note that the models will each be trained on a single Spark executor so some caution may be necessary to not blow up the executor memory if the data within each group is too large for a single executor to hold and do the model training in memory.</p>]]></content><author><name></name></author><category term="python,"/><category term="pyspark,"/><category term="machine-learning,"/><category term="pandas"/><summary type="html"><![CDATA[Say you find yourself in the peculiar situation where you need to train a whole bunch of scikit-learn models over different groups from a large amount of data. And say you want to leverage Spark to distribute the process to do it all in a scalable fashion.]]></summary></entry></feed>